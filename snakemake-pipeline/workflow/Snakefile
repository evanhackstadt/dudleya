import os

# Config:
# configfile specified in profile (config/config.yaml)

SAMPLES = config["samples"].keys()     # maps sample name (POPCODE_LP_xxx_Du-xxx) to r1 and r2 filepaths
DATA_DIR = config["data_parent_dir"]

# ---- Overall Goal: ----
rule all:
    input:
        "results/pca/pca.png"


# ---- Rule 1: Fastp Filtering ----
rule fastp:
    input:
        r1 = lambda wildcards: config["samples"][wildcards.sample]["r1"],
        r2 = lambda wildcards: config["samples"][wildcards.sample]["r2"]
    output:
        r1 = os.path.join(DATA_DIR, "filtered_reads", "{sample}_R1.fastq.gz"), # store next to raw reads rather than locally
        r2 = os.path.join(DATA_DIR, "filtered_reads", "{sample}_R2.fastq.gz"),
        html = "results/qc/FastpFilterReport_{sample}.html",    # but store reports locally
        json = "results/qc/FastpFilterReport_{sample}.json"
    threads: 4
    resources:
        mem_mb = lambda wc, input: max(2.5 * input.size_mb, 2000),    # dynamically allocate mem for large inputs
        cpus_per_task = lambda wildcards, threads: threads  # dynamically allocate # cpus = # threads
    shell:
        """
        fastp -i {input.r1} -I {input.r2} \
              -o {output.r1} -O {output.r2} \
              --trim_poly_g \
              --length_required 100 \
              -q 20 \
              --thread {threads} \
              --html {output.html} \
              --json {output.json}
        """

# ---- Rule 2: BWA Mapping & Samtools Sort ----
rule map_and_sort:
    input:
        ref = config["ref_genome"],
        r1 = os.path.join(DATA_DIR, "filtered_reads", "{sample}_R1.fastq.gz"),
        r2 = os.path.join(DATA_DIR, "filtered_reads", "{sample}_R2.fastq.gz")
    output:
        bam = os.path.join(DATA_DIR, "mapped_reads", "{sample}_sorted.bam")
    log:
        "logs/bwa_mem_samtools/{sample}.log"
    threads: 16
    resources:
        slurm_partition = "condo",
        runtime = 360,      # 6 hours
        mem_mb = 32000,     # 32 GB RAM
        cpus_per_task = lambda wildcards, threads: threads
    benchmark:
        "benchmarks/bwa_mem_samtools/{sample}.tsv"
    shell:
        # MANUAL thread allocation (12 for BWA, 4 for Sam). Update if reducing threads.
        """
        bwa mem -t 12 {input.ref} {input.r1} {input.r2} | \
        samtools sort -@ 4 -o {output.bam}
        """

# ---- Rule 3: Indexing ----
rule samtools_index:
    input:
        os.path.join(DATA_DIR, "mapped_reads", "{sample}_sorted.bam")
    output:
        os.path.join(DATA_DIR, "mapped_reads", "{sample}_sorted.bam.bai")
    shell:
        "samtools index {input}"

# ---- Rule 4: Flagstat ----
rule samtools_flagstat:
    input:
        os.path.join(DATA_DIR, "mapped_reads", "{sample}_sorted.bam")
    output:
        "results/qc/{sample}_flagstat.txt"
    shell:
        "samtools flagstat {input} > {output}"


# ---- Rule 5: Create BAM Filelist
# The "aggregate" or "gather" step. Requires that ALL samples have been processed.
rule create_bam_list:
    input:
        bams = expand(os.path.join(DATA_DIR, "mapped_reads", "{sample}_sorted.bam"), sample=SAMPLES),
        indexes = expand(os.path.join(DATA_DIR, "mapped_reads", "{sample}_sorted.bam.bai"), sample=SAMPLES)
    output:
        bam_list = "results/angsd/bam.filelist"
    run:
        with open(output.bam_list, 'w') as f:
            for bam in input.bams:
                f.write(str(bam) + "\n")

# ---- Rule 6: ANGSD ----
rule angsd:
    input:
        bam_list = "results/angsd/bam.filelist",
        ref = config["ref_genome"],
        anc = config["anc_genome"]
    output:
        beagle = "results/angsd/population.beagle.gz",
        mafs = "results/angsd/population.mafs.gz"
        # additional output files not listed here will be created
    log:
        "logs/angsd/angsd.log"
    threads: 8
    resources:
        slurm_partition = "mem",
        runtime = 1440,      # 24 hours
        mem_mb = 256000,      # 256 GB RAM
        cpus_per_task = lambda wildcards, threads: threads
    benchmark:
        "benchmarks/angsd/angsd.tsv"
    shell:
        """
        angsd -b {input.bam_list} -ref {input.ref} -anc {input.anc} \
              -GL 1 -doGlf 2 -doMajorMinor 1 -doCounts 1 -doMaf 1 \
              -doSaf 1 -doPost 1 -doGeno 32 -uniqueOnly 1 \
              -minMapQ 30 -minQ 20 \
              -SNP_pval 1e-6 -trim 5 \
              -nThreads {threads} \
              -out results/angsd/population
        """

# ---- Rule 7: PCAngsd ----
rule pcangsd:
    input:
        "results/angsd/population.beagle.gz"
    output:
        "results/pca/population.cov"
    threads: 8
    resources:
        slurm_partition = "mem",
        runtime = 1440,      # 24 hours
        mem_mb = lambda wc, input: max(20 * input.size_mb, 4000),    # dynamically allocate mem
        cpus_per_task = lambda wildcards, threads: threads
    shell:
        """
        pcangsd -b {input} -o results/pca/population -t {threads}
        """

# ---- Rule 8: Create Population Info ----
rule create_pop_info:
    input:
        "results/pca/population.cov"    # this just links it to rule 7
    output:
        info = "results/pca/population.info"
    run:
        with open(output.info, "w") as f:
            for sample in SAMPLES:
                f.write(sample + "\n")

# ---- Rule 9: Visualization ----
rule plot_pca:
    input:
        cov = "results/pca/population.cov",
        info = "results/pca/population.info"
    output:
        "results/pca/pca.png"
    conda:
        "/WAVE/projects/whittalllab/conda_envs/visualization"
    script:
        "scripts/pcangsd_visualize.py"